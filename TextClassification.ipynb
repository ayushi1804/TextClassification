{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic={}\n",
    "#tokenizer to remove all the punctuation and breaking the string into words\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#set of all the english stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simply adding in dictionary\n",
    "def add_to_dic(filtrd):\n",
    "    for l in filtrd:\n",
    "        #if already prsent simply increment\n",
    "        if l in dic:\n",
    "            dic[l]=dic[l]+1\n",
    "        #else initialise\n",
    "        else:\n",
    "            dic[l]=1\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the file given at this path\n",
    "#I am simply reading the file and \n",
    "#removing stopwords and punctuation\n",
    "def op_on_files(path):\n",
    "    \n",
    "    file= open(path,'r')\n",
    "    st= file.read()\n",
    "    #Removing punctuation and converting the string to a list of words\n",
    "    strng=tokenizer.tokenize(st)\n",
    "    #removing stopwords\n",
    "    filtrd = [w for w in strng if not w in stop_words]\n",
    "    \n",
    "    #adding the words in dictionary\n",
    "    add_to_dic(filtrd)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in this function I am simply traversing the list of files and\n",
    "#calling op_on_files on each individual file \n",
    "def fun(files,root):\n",
    "    for f in files:\n",
    "        #I have done this because I have saved this project file\n",
    "        #in the same directory where all these text files are stored\n",
    "        #so for once *files* will contain a single file i.e. this ipynb file\n",
    "        #so to avoid this file I have done this\n",
    "        if f.endswith(\"ipynb\"):\n",
    "            continue\n",
    "                \n",
    "        #path for each file\n",
    "        path= root+\"/\"+f\n",
    "        #further calling for performing operations on each file\n",
    "        op_on_files(path)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#by using os.walk traversing the files directory\n",
    "#calling the function fun() to do the required operations on each file\n",
    "#here *files* is a list of files in each directory\n",
    "#root is a string\n",
    "for root, dirs, files in os.walk(\"./\", topdown=True):\n",
    "    fun(files,root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244731"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as dictionaries cannot be sorted I am converting it into a list *d*\n",
    "#this list contains 2 columns, first the count then the word\n",
    "k= dic.keys()\n",
    "d=[]\n",
    "for i in k:\n",
    "    d.append([dic[i],i])\n",
    "    \n",
    "#sorting the list\n",
    "d= sorted(d, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= list(dic.values())\n",
    "x= list(range(len(dic.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting the points to decide how many points to take in our vocab\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x,y)\n",
    "#plt.axis([0,4000,0,60000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#features contains the final set of 2000 words that will be our vocab\n",
    "features=[]\n",
    "for i in range(0,2000):\n",
    "    features.append(d[i][1])\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simply adding the string filtered words in X if they are in our vocab i.e. *features*\n",
    "#here j is the row index\n",
    "def addToX(filtered,features,j,X):\n",
    "    for s in filtered:\n",
    "        if s in features:\n",
    "            id= features.index(s)\n",
    "            X[j][id]= X[j][id]+1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Again for each file in files\n",
    "#I am simply reading the file and \n",
    "#removing stopwords and punctuation\n",
    "def file_to_X(files,X,features,root,j):\n",
    "    \n",
    "    for f in files:\n",
    "        path= root+'/'+f\n",
    "        fle= open(path,'r')\n",
    "        st= fle.read()\n",
    "        strng=tokenizer.tokenize(st)\n",
    "        filtrd = [w for w in strng if not w in stop_words]\n",
    "        \n",
    "        #Then I am adding the final list of words to X\n",
    "        X=addToX(filtrd,features,j,X)\n",
    "        #this is to keep track of the row nmber in X\n",
    "        j=j+1\n",
    "        #print(X.shape, x.shape)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making our data array\n",
    "def setIntoXForm(X, features):\n",
    "    #i is simply to keep track on which class\n",
    "    i=-1    \n",
    "    #traversing the files directory\n",
    "    for root, dirs, files in os.walk(\"./\", topdown=True):\n",
    "        #I have done this because I have saved this project file\n",
    "        #in the same directory where all these text files are stored\n",
    "        #so for once *files* will contain a single file i.e. this ipynb file\n",
    "        #so to avoid this file I have done this\n",
    "        if len(files)==1:\n",
    "            continue\n",
    "        \n",
    "        i+=1\n",
    "        X=file_to_X(files,X,features,root,1000*i)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialising data array and then calling setIntoXForm function\n",
    "data= np.zeros([20000,2000])\n",
    "data= setIntoXForm(data, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y is the output array\n",
    "Y= np.zeros(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y contains 1000 1's, 1000 2's upto.... 1000 19's just to represent the output classes\n",
    "for i in range(20):\n",
    "    for j in range(1000):\n",
    "        Y[(i*1000)+j]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test= model_selection.train_test_split(data, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape\n",
    "n= X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(753, 2000)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[Y_train==0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the vocab dictionary \n",
    "vocab={}\n",
    "classes= [x for x in range(0,20)]\n",
    "#Traversing in all classes from 0 to 19\n",
    "for c in classes:\n",
    "    vocab[c]={}\n",
    "    #Taking out the rows that belong to class c\n",
    "    X_x= X_train[Y_train==c]\n",
    "    vocab[c][\"total\"]= X_x.sum()\n",
    "    #print(vocab[c][\"total\"], X_x.sum())\n",
    "    #For the class c traversing in all the features and computing the values\n",
    "    for f in range(len(features)):   \n",
    "        vocab[c][f]= X_x[:,f].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traversing on all the features and adding the log probabilities\n",
    "def find_prob(X,c):\n",
    "    p=0\n",
    "    total_features= len(features)\n",
    "    #Looping over all the features and adding the log probablities of each feature\n",
    "    #probabilty of each feature will be that word count in that class divided by total word count\n",
    "    #also included laplace correction\n",
    "    for f in range(total_features):\n",
    "        #Mulitiplying X[f] for more accurate results\n",
    "        #No need to check whether X[f] is 0 or not as for zero , the whole probability will be zero\n",
    "        p=p+ X[f]*np.log((vocab[c][f]+1)/(vocab[c][\"total\"]+total_features))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For a given row in X_test finding the maximum probability among all the classes\n",
    "def predict_X(X):\n",
    "    #initialising max_prb with a minimum value\n",
    "    max_prb= -sys.maxsize -1\n",
    "    #i will store the result\n",
    "    i=-1\n",
    "    for c in classes:\n",
    "        prob= find_prob(X,c)\n",
    "        if prob>max_prb:\n",
    "            max_prb=prob\n",
    "            i= c\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For all the test points calculating y-predicted\n",
    "def pred(X_test):\n",
    "    y_pred=np.zeros(X_test.shape[0])\n",
    "    for i in range(X_test.shape[0]):\n",
    "        y_pred[i]= predict_X(X_test[i])\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 8., 0., ..., 7., 3., 7.])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred= pred(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[189   0   0   0   0   0   2   5   4   1   0   0   2   3   1   0   1   1\n",
      "    0  38]\n",
      " [  0 185   0  16  24   4   8   1   0   0   0   2   7   3   0   0   0   0\n",
      "    0   0]\n",
      " [  0  24  14 135  21  42   9   3   0   0   0   0  11   2   0   0   0   0\n",
      "    0   0]\n",
      " [  0   2   0 221  18   1   6   0   0   0   0   0   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   4   1  12 231   1   6   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0  26   1  13   8 205   3   2   0   0   0   1   6   0   3   0   0   1\n",
      "    0   0]\n",
      " [  0   1   0   1   4   0 206   8   2   1   0   0   3   0   0   0   0   0\n",
      "    0   1]\n",
      " [  0   0   0   1   1   0  10 227   9   1   0   0   8   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   1   0   0   0   0   5   8 236   1   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   1   3   7 214   8   0   0   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   1   0   1   6   4  50 187   0   1   0   1   0   0   0\n",
      "    1   0]\n",
      " [  1   0   0   1   0   2   0   0   0   0   0 248   8   3   0   0   2   0\n",
      "    0   0]\n",
      " [  0   3   0   4   5   0   8   5   0   0   0   0 219   1   2   0   0   0\n",
      "    0   0]\n",
      " [  0   2   0   0   2   0   1   5   2   0   0   0  11 227   1   0   0   0\n",
      "    2   0]\n",
      " [  0   4   0   1   0   1   1   0   2   0   1   2  15   1 220   0   0   0\n",
      "    1   0]\n",
      " [  2   0   0   0   0   0   0   0   0   0   0   0   1   0   0 236   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   1   5   0   0   1   0   0   1   0 222   1\n",
      "    8   9]\n",
      " [  0   0   0   0   3   0   3   4   1   1   0   1   1   0   1   0   9 211\n",
      "   21   3]\n",
      " [  0   0   0   0   0   0   2   1   0   0   1   1   0   0   6   1  38   7\n",
      "  178  26]\n",
      " [ 46   0   0   0   0   0   5   0   1   0   0   0   1   0   1   1  15   2\n",
      "   19 136]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.79      0.77      0.78       247\n",
      "        1.0       0.73      0.74      0.74       250\n",
      "        2.0       0.88      0.05      0.10       261\n",
      "        3.0       0.55      0.89      0.68       249\n",
      "        4.0       0.73      0.91      0.81       255\n",
      "        5.0       0.80      0.76      0.78       269\n",
      "        6.0       0.74      0.91      0.82       227\n",
      "        7.0       0.81      0.88      0.85       257\n",
      "        8.0       0.86      0.94      0.90       251\n",
      "        9.0       0.80      0.91      0.85       234\n",
      "       10.0       0.95      0.74      0.83       252\n",
      "       11.0       0.97      0.94      0.95       265\n",
      "       12.0       0.74      0.89      0.81       247\n",
      "       13.0       0.95      0.90      0.92       253\n",
      "       14.0       0.92      0.88      0.90       249\n",
      "       15.0       0.99      0.99      0.99       239\n",
      "       16.0       0.77      0.90      0.83       248\n",
      "       17.0       0.95      0.81      0.88       259\n",
      "       18.0       0.77      0.68      0.73       261\n",
      "       19.0       0.64      0.60      0.62       227\n",
      "\n",
      "avg / total       0.82      0.80      0.79      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Print the classification Report of predicted results\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(Y_test, y_pred))\n",
    "print(classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making an object of inbuilt multinomial naive bayes\n",
    "#an fitting our data\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.,  8.,  0., 17.,  3.,  4., 11.,  7., 19., 17., 11.])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#storing the predicted results in Y_pred_inbuilt\n",
    "Y_pred_inbuilt= clf.predict(X_test)\n",
    "Y_pred_inbuilt[0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8022"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[189   0   0   0   0   0   2   5   4   1   0   0   2   3   1   0   1   1\n",
      "    0  38]\n",
      " [  0 185   0  16  24   4   8   1   0   0   0   2   7   3   0   0   0   0\n",
      "    0   0]\n",
      " [  0  24  14 135  21  42   9   3   0   0   0   0  11   2   0   0   0   0\n",
      "    0   0]\n",
      " [  0   2   0 221  18   1   6   0   0   0   0   0   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   4   1  12 231   1   6   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0  26   1  13   8 205   3   2   0   0   0   1   6   0   3   0   0   1\n",
      "    0   0]\n",
      " [  0   1   0   1   4   0 206   8   2   1   0   0   3   0   0   0   0   0\n",
      "    0   1]\n",
      " [  0   0   0   1   1   0  10 227   9   1   0   0   8   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   1   0   0   0   0   5   8 236   1   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   1   3   7 214   8   0   0   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   1   0   1   6   4  51 186   0   1   0   1   0   0   0\n",
      "    1   0]\n",
      " [  1   0   0   1   0   2   0   0   0   0   0 248   8   3   0   0   2   0\n",
      "    0   0]\n",
      " [  0   3   0   4   5   0   8   5   0   0   0   0 219   1   2   0   0   0\n",
      "    0   0]\n",
      " [  0   2   0   0   2   0   1   5   2   0   0   0  11 227   1   0   0   0\n",
      "    2   0]\n",
      " [  0   4   0   1   0   1   1   0   2   0   1   2  15   1 220   0   0   0\n",
      "    1   0]\n",
      " [  0   0   0   0   0   0   2   0   0   0   0   0   1   0   0 236   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   1   5   0   0   1   0   0   1   0 222   1\n",
      "    8   9]\n",
      " [  0   0   0   0   3   0   3   4   1   1   0   1   1   0   1   0   9 211\n",
      "   21   3]\n",
      " [  0   0   0   0   0   0   2   1   0   0   1   1   0   0   6   1  38   7\n",
      "  178  26]\n",
      " [ 46   0   0   0   0   0   5   0   1   0   0   0   1   0   1   1  15   2\n",
      "   19 136]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.80      0.77      0.78       247\n",
      "        1.0       0.73      0.74      0.74       250\n",
      "        2.0       0.88      0.05      0.10       261\n",
      "        3.0       0.55      0.89      0.68       249\n",
      "        4.0       0.73      0.91      0.81       255\n",
      "        5.0       0.80      0.76      0.78       269\n",
      "        6.0       0.74      0.91      0.81       227\n",
      "        7.0       0.81      0.88      0.85       257\n",
      "        8.0       0.86      0.94      0.90       251\n",
      "        9.0       0.79      0.91      0.85       234\n",
      "       10.0       0.95      0.74      0.83       252\n",
      "       11.0       0.97      0.94      0.95       265\n",
      "       12.0       0.74      0.89      0.81       247\n",
      "       13.0       0.95      0.90      0.92       253\n",
      "       14.0       0.92      0.88      0.90       249\n",
      "       15.0       0.99      0.99      0.99       239\n",
      "       16.0       0.77      0.90      0.83       248\n",
      "       17.0       0.95      0.81      0.88       259\n",
      "       18.0       0.77      0.68      0.73       261\n",
      "       19.0       0.64      0.60      0.62       227\n",
      "\n",
      "avg / total       0.82      0.80      0.79      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(Y_test, Y_pred_inbuilt))\n",
    "print(classification_report(Y_test, Y_pred_inbuilt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
